{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# https://sandeep14.medium.com/running-graphrag-locally-with-neo4j-and-ollama-text-format-371bf88b14b7\n",
    "\n",
    "%pip install --upgrade --quiet  langchain langchain-community langchain-ollama langchain-experimental neo4j tiktoken yfiles_jupyter_graphs python-dotenv fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from neo4j import GraphDatabase, Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setting Up the Neo4j Graph\n",
    "First, we need to initialize the connection to the Neo4j graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_graph():\n",
    "    \"\"\"Initialize Neo4j graph connection\"\"\"\n",
    "    return Neo4jGraph(\n",
    "    url= \"bolt://localhost:7687\" ,\n",
    "    username=\"neo4j\", #default\n",
    "    password=\"your_password\" #change accordingly\n",
    "    )\n",
    "\n",
    "def clear_database(graph):\n",
    "    \"\"\"Clear all nodes, relationships, and vector indexes from the Neo4j database\"\"\"\n",
    "    # First drop the vector index\n",
    "    try:\n",
    "        graph.query(\"\"\"\n",
    "            DROP INDEX vector IF EXISTS\n",
    "        \"\"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Vector index drop attempt resulted in: {e}\")\n",
    "    \n",
    "    # Then delete all nodes and relationships\n",
    "    graph.query(\"\"\"\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n\n",
    "    \"\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Text to Graph Conversion with Ollama\n",
    "\n",
    "We start with a block of text that we want to convert into a graph. For this example, we’ll use a biographical snippet of Marie Curie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "She was, in 1906, the first woman to become a professor at the University of Paris. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ingestion handles the conversion of this text into graph documents, which are then added to the Neo4j database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(text: str, llm_model: str = llm_model):\n",
    "    \"\"\"Ingest text data into Neo4j graph and create vector embeddings\"\"\"\n",
    "    # Initialize graph\n",
    "    graph = init_graph()\n",
    "    \n",
    "    # Clear existing data\n",
    "    clear_database(graph)\n",
    "    \n",
    "    # Convert text to documents\n",
    "    documents = [Document(page_content=text)]\n",
    "    \n",
    "    # Initialize LLM for text-to-graph conversion\n",
    "    llm = ChatOllama(model=llm_model, temperature=0)\n",
    "    llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "    \n",
    "    # Convert text to graph documents\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "    # Add the generated graph into Neo4j\n",
    "    graph.add_graph_documents(\n",
    "        graph_documents,\n",
    "        baseEntityLabel=True,\n",
    "        include_source=True\n",
    "    )\n",
    "   \n",
    "    # Create vector embeddings\n",
    "    embed = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    vector_index = Neo4jVector.from_existing_graph(\n",
    "        embedding=embed,\n",
    "        search_type=\"hybrid\",\n",
    "        node_label=\"Document\",\n",
    "        text_node_properties=[\"text\"],\n",
    "        embedding_node_property=\"embedding\"\n",
    "    )\n",
    "    \n",
    "    return graph, vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Querying Entities with Neo4j\n",
    "\n",
    "Once we have the graph stored in Neo4j, we can run queries against it. The querying_neo4j function takes in a user’s question, extracts entities using the LLM, and retrieves relationships from the graph database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    \"\"\"Model for extracted entities\"\"\"\n",
    "    names: list[str] = Field(..., description=\"All entities from the text\")\n",
    "\n",
    "def create_entity_chain(llm_model: str = llm_model):\n",
    "    \"\"\"Create entity extraction chain\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Extract organization and person entities from the text.\"),\n",
    "        (\"human\", \"Extract entities from: {question}\")\n",
    "    ])\n",
    "    \n",
    "    # llm = OllamaFunctions(model=llm_model, format=\"json\", temperature=0)\n",
    "    llm = OllamaFunctions(model=llm_model, format=\"json\", temperature=0)\n",
    "    return prompt | llm.with_structured_output(Entities)  # Removed include_raw=True\n",
    "\n",
    "def graph_retriever(question: str, graph: Neo4jGraph, entity_chain) -> str:\n",
    "    \"\"\"Retrieve relationships for entities from Neo4j\"\"\"\n",
    "    try:\n",
    "        # Get entities directly from the structured output\n",
    "        response = entity_chain.invoke({\"question\": question})\n",
    "        entities = response.names  # Access names directly from the Entities model\n",
    "        print(\"Retrieved Entities:\", entities)\n",
    "        \n",
    "        results = []\n",
    "        for entity in entities:\n",
    "            query_response = graph.query(\n",
    "                \"\"\"\n",
    "                MATCH (p:Person {id: $entity})-[r]->(e)\n",
    "                RETURN p.id AS source_id, type(r) AS relationship, e.id AS target_id\n",
    "                LIMIT 50\n",
    "                \"\"\",\n",
    "                {\"entity\": entity}\n",
    "            )\n",
    "            results.extend([\n",
    "                f\"{el['source_id']} - {el['relationship']} -> {el['target_id']}\"\n",
    "                for el in query_response\n",
    "            ])\n",
    "        \n",
    "        return \"\\n\".join(results) if results else \"No relationships found.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in graph_retriever: {e}\")\n",
    "        return \"Error retrieving relationships.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Hybrid Search with Ollama and Graph Data\n",
    "\n",
    "Finally, we combine both graph-based retrieval and embeddings to perform hybrid searches. The querying_ollama function allows users to query based on both the graph relationships and embedding-based context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_chain(graph: Neo4jGraph, vector_retriever, entity_chain, llm_model: str = llm_model):\n",
    "    \"\"\"Create question-answering chain\"\"\"\n",
    "    def full_retriever(question: str):\n",
    "        graph_data = graph_retriever(question, graph, entity_chain)\n",
    "        vector_data = [el.page_content for el in vector_retriever.invoke(question)]\n",
    "        return f\"Graph data: {graph_data}\\nVector data: {'#Document '.join(vector_data)}\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    llm = ChatOllama(model=llm_model, temperature=0)\n",
    "    \n",
    "    return (\n",
    "        {\n",
    "            \"context\": lambda input: full_retriever(input),\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function first retrieves both the graph data and the vector-based embeddings, then uses the retrieved context to generate a concise answer to the user’s query.\n",
    "\n",
    "# Step 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Entities: ['Marie Curie', 'Pierre Curie']\n",
      "Final Answer: Marie Curie and Pierre Curie were a married couple who made significant contributions to science. They were both physicists and chemists who conducted pioneering research on radioactivity, which led to numerous accolades.\n",
      "\n",
      "Marie Curie was a trailblazer in many ways:\n",
      "\n",
      "* She was the first woman to win a Nobel Prize (in 1906) and the first person to win it twice.\n",
      "* She was the only person to win a Nobel Prize in two scientific fields.\n",
      "* She was the first woman to become a professor at the University of Paris, achieving this milestone in 1906.\n",
      "\n",
      "Pierre Curie, on the other hand, was Marie's husband and a co-winner of her first Nobel Prize. He was also a physicist and chemist who made significant contributions to the field of radioactivity.\n",
      "\n",
      "Together, they formed a remarkable scientific partnership that paved the way for future generations of scientists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Initialize components\n",
    "    \n",
    "    text = \"\"\"\n",
    "    Marie Curie, born in 1867, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\n",
    "    She was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\n",
    "    Her husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\n",
    "    She was, in 1906, the first woman to become a professor at the University of Paris.\n",
    "    \"\"\"\n",
    "    \n",
    "    graph = init_graph()\n",
    "\n",
    "   # Clear the database first\n",
    "    clear_database(graph)\n",
    "\n",
    "\n",
    "    # Ingest data\n",
    "    graph, vector_retriever = ingest_data(text)\n",
    "    \n",
    "    # Create chains\n",
    "    entity_chain = create_entity_chain()\n",
    "    qa_chain = create_qa_chain(graph, vector_retriever, entity_chain)\n",
    "    \n",
    "    # Test the chain\n",
    "    question = \"Who are Marie Curie and Pierre Curie?\"\n",
    "    response = qa_chain.invoke(question)\n",
    "    print(\"Final Answer:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

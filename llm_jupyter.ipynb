{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/how_to/#extraction\n",
    "# https://python.langchain.com/docs/tutorials/extraction/\n",
    "%pip install --upgrade --quiet typing langchain langchain_core pydantic langchain-community langchain_experimental langchain_ollama neo4j langchain_neo4j yfiles_jupyter_graphs jupyterlab ipywidgets jupyterlab-widgets langchain_ollama llama-cpp-python json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate neo4j instance.\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Schema\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the extractor\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_4224\\3466569965.py:7: LangChainDeprecationWarning: The class `OllamaFunctions` was deprecated in LangChain 0.0.64 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = OllamaFunctions(model=llm_model, temperature=0, format=\"json\")\n"
     ]
    }
   ],
   "source": [
    "#Load LLM\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "llm_model=\"mistral:7b-instruct-v0.3-q8_0\" #mistral:7b-instruct-v0.3-q8_0, llama3.1:8b-instruct-q8_0, llama3.2:3b-instruct-fp16\n",
    "\n",
    "llm = OllamaFunctions(model=llm_model, temperature=0, format=\"json\")\n",
    "# llm = ChatOllama(model=llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading via LlammaCPP\n",
    "# # from langchain_community.llms import LlamaCpp\n",
    "# from langchain_community.chat_models import ChatLlamaCpp\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "# # https://github.com/langchain-ai/langchain/issues/25318\n",
    "# # https://github.com/langchain-ai/langchainjs/issues/6051\n",
    "# llm = ChatLlamaCpp(\n",
    "#     model_path=\"/home/ubuntu/repos/gguf/llama3_1_8b_f16.gguf\", #\"/home/ubuntu/repos/gguf/llama3_2_3b_q8_0.gguf\", mistral_7b_f16.gguf, llama3_1_8b_f16.gguf\n",
    "#     n_gpu_layers=-1,\n",
    "#     temperature=0,\n",
    "#     echo='True',\n",
    "#     grammar='json',\n",
    "#     n_batch=1,\n",
    "#     n_ctx=2048,\n",
    "#     f16_kv=False,\n",
    "#     # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#     # verbose=True,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters=None), Person(name='Anna', hair_color='black', height_in_meters=None)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text instead\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(file_path=\"dummytext.txt\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #or TokenTextSplitter\n",
    "documents = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "#see https://python.langchain.com/docs/how_to/extraction_long_text/ for more efficient parallel runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw unstructured text into graph documents.\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "llm_transformer = LLMGraphTransformer(llm=llm) #, ignore_tool_usage=True\n",
    "\n",
    "#if filtering is required.\n",
    "# llm_transformer_filtered = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Country\", \"Organization\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n",
    "# )\n",
    "# graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "#     documents\n",
    "# )\n",
    "# print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents_filtered[0].relationships}\")\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_documents:nodes=[Node(id='Giovanni Caruso', type='Person', properties={}), Node(id='Maria', type='Person', properties={}), Node(id='Santa Caterina', type='Location', properties={}), Node(id='Sicily', type='Location', properties={}), Node(id='Caruso Family', type='Family', properties={})] relationships=[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Maria', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Giovanni Caruso', type='Person', properties={}), type='FAMILY_MEMBER', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Maria', type='Person', properties={}), type='FAMILY_MEMBER', properties={})] source=Document(metadata={'source': 'dummytext.txt'}, page_content=\"1. The Story of Amicoâ€™s Family: A Legacy of Love and Tradition\\nIn the idyllic village of Santa Caterina, amidst the rolling hills and sun-kissed landscapes of Sicily, lies the genesis of the Caruso family, a lineage intertwined with the island's rich culinary tapestry. The Carusos were not mere inhabitants of the land; they were the keepers of a culinary heritage that spanned generations. Each family member contributed their unique flair, crafting a narrative of flavors that reflected their diverse experiences and deep-seated love for food.\\n\\nGiovanni Caruso and Maria: The Founding Generation\")\n",
      "Nodes:[Node(id='Giovanni Caruso', type='Person', properties={}), Node(id='Maria', type='Person', properties={}), Node(id='Santa Caterina', type='Location', properties={}), Node(id='Sicily', type='Location', properties={}), Node(id='Caruso Family', type='Family', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Maria', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Giovanni Caruso', type='Person', properties={}), type='FAMILY_MEMBER', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Maria', type='Person', properties={}), type='FAMILY_MEMBER', properties={})]\n",
      "[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Maria', type='Person', properties={}), type='MARRIED', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='RESIDED_IN', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Giovanni Caruso', type='Person', properties={}), type='FAMILY_MEMBER', properties={}), Relationship(source=Node(id='Caruso Family', type='Family', properties={}), target=Node(id='Maria', type='Person', properties={}), type='FAMILY_MEMBER', properties={})]\n",
      "[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Maria', type='Person', properties={}), type='SPOUSE', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='RESIDES_IN', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Olives', type='Product', properties={}), type='PRODUCES', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Grapes', type='Product', properties={}), type='PRODUCES', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Oils', type='Product', properties={}), type='PRODUCES', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Wines', type='Product', properties={}), type='PRODUCES', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Kitchen', type='Location', properties={}), type='WORKS_IN', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Stews', type='Food', properties={}), type='COOKS', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Pastries', type='Food', properties={}), type='COOKS', properties={})]\n"
     ]
    }
   ],
   "source": [
    "#view the contents\n",
    "print(f\"graph_documents:{graph_documents[0]}\")\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")\n",
    "\n",
    "doc = graph_documents[0:2]\n",
    "non_empty_nodes = [doc for doc in doc if doc.nodes]\n",
    "non_empty_rel = [doc for doc in doc if doc.relationships]\n",
    "\n",
    "# for graph_document in non_empty_nodes:\n",
    "#     print(graph_document.nodes)\n",
    "\n",
    "for doc in non_empty_rel:\n",
    "    print(doc.relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "# Add contents into neo4j database\n",
    "driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
    "    )\n",
    "\n",
    "def clear_database(tx):\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n\n",
    "    \"\"\")    \n",
    "\n",
    "with driver.session() as session:\n",
    "    session.execute_write(clear_database)\n",
    "\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652ff7193f5346d680cba4562c42cc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r]->(t) WHERE type(r) <> 'MENTIONS'RETURN s,r,t\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "- **Document**\n",
      "  - `id`: STRING Example: \"28e6187d418c3194dccd784a471816f7\"\n",
      "  - `source`: STRING Available options: ['dummytext.txt']\n",
      "  - `text`: STRING Example: \"Conclusion The Caruso family's story is a rich tap\"\n",
      "- **Person**\n",
      "  - `id`: STRING Example: \"Giovanni Caruso\"\n",
      "- **Location**\n",
      "  - `id`: STRING Available options: ['Santa Caterina', 'Sicily', 'Village', 'Italy', 'Tuscany', 'New York City', 'Los Angeles', 'Rome']\n",
      "- **Family**\n",
      "  - `id`: STRING Available options: ['Caruso Family', 'Caruso_Family']\n",
      "- **Place**\n",
      "  - `id`: STRING Available options: ['Kitchen', 'Trattoria', 'Community Kitchens']\n",
      "- **Food**\n",
      "  - `id`: STRING Available options: ['Caponata', 'Fresh_Pasta']\n",
      "- **Restaurant**\n",
      "  - `id`: STRING Available options: ['La Dolce Vita', 'La Terra Di Siena', \"Amico'S\", 'Bella Vita']\n",
      "- **Program**\n",
      "  - `id`: STRING Available options: ['Mentorship Programs']\n",
      "- **Organization**\n",
      "  - `id`: STRING Available options: ['Caruso Family', 'Carusos', 'Local_Farmers_And_Producers']\n",
      "- **Initiative**\n",
      "  - `id`: STRING Available options: ['Farm-To-Table_Initiative']\n",
      "- **Concept**\n",
      "  - `id`: STRING Available options: ['Environmental_Sustainability', 'Local_Economy', 'Cultural_Diversity', 'Art', 'Food']\n",
      "- **Produce**\n",
      "  - `id`: STRING Available options: ['Seasonal_Produce', 'Organic_Produce']\n",
      "- **Persongroup**\n",
      "  - `id`: STRING Available options: ['Homeless', 'Low-Income Families', 'Patrons']\n",
      "- **Event**\n",
      "  - `id`: STRING Available options: ['Exhibitions', 'Events', 'Culinary Workshops', 'Food Drives', 'Live_Performances', 'Cooking_Demonstrations']\n",
      "- **Organizationgroup**\n",
      "  - `id`: STRING Available options: ['Local Businesses']\n",
      "- **Culture**\n",
      "  - `id`: STRING Available options: ['Sicilian_Culture']\n",
      "- **Cuisine**\n",
      "  - `id`: STRING Available options: ['International_Cuisines']\n",
      "- **Market**\n",
      "  - `id`: STRING Available options: ['Artisan_Markets']\n",
      "- **Group**\n",
      "  - `id`: STRING Available options: ['Local Artisans', 'Performers', 'Local Artists']\n",
      "- **Artwork**\n",
      "  - `id`: STRING Available options: ['Paintings', 'Sculptures', 'Ceramics']\n",
      "- **Artform**\n",
      "  - `id`: STRING Available options: ['Music']\n",
      "- **Exchange**\n",
      "  - `id`: STRING Available options: ['Food_Exchange']\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Document)-[:MENTIONS]->(:Location)\n",
      "(:Document)-[:MENTIONS]->(:Family)\n",
      "(:Document)-[:MENTIONS]->(:Organization)\n",
      "(:Document)-[:MENTIONS]->(:Person)\n",
      "(:Document)-[:MENTIONS]->(:Place)\n",
      "(:Document)-[:MENTIONS]->(:Food)\n",
      "(:Document)-[:MENTIONS]->(:Restaurant)\n",
      "(:Document)-[:MENTIONS]->(:Event)\n",
      "(:Document)-[:MENTIONS]->(:Program)\n",
      "(:Document)-[:MENTIONS]->(:Produce)\n",
      "(:Document)-[:MENTIONS]->(:Concept)\n",
      "(:Document)-[:MENTIONS]->(:Initiative)\n",
      "(:Document)-[:MENTIONS]->(:Persongroup)\n",
      "(:Document)-[:MENTIONS]->(:Organizationgroup)\n",
      "(:Document)-[:MENTIONS]->(:Exchange)\n",
      "(:Document)-[:MENTIONS]->(:Culture)\n",
      "(:Document)-[:MENTIONS]->(:Market)\n",
      "(:Document)-[:MENTIONS]->(:Cuisine)\n",
      "(:Document)-[:MENTIONS]->(:Artform)\n",
      "(:Document)-[:MENTIONS]->(:Artwork)\n",
      "(:Document)-[:MENTIONS]->(:Group)\n",
      "(:Person)-[:MARRIED]->(:Person)\n",
      "(:Person)-[:RESIDED_IN]->(:Location)\n",
      "(:Person)-[:SPOUSE]->(:Person)\n",
      "(:Person)-[:RESIDES_IN]->(:Location)\n",
      "(:Person)-[:WORKS_IN]->(:Place)\n",
      "(:Person)-[:CHILD]->(:Person)\n",
      "(:Person)-[:RESIDENCE]->(:Location)\n",
      "(:Person)-[:TRAVEL]->(:Location)\n",
      "(:Person)-[:CREATED]->(:Person)\n",
      "(:Person)-[:CAPTIVATED]->(:Location)\n",
      "(:Person)-[:GO_TO]->(:Location)\n",
      "(:Person)-[:CREATED_DISHES]->(:Person)\n",
      "(:Person)-[:IS]->(:Person)\n",
      "(:Person)-[:OWNED]->(:Restaurant)\n",
      "(:Person)-[:RELATIVE_OF]->(:Person)\n",
      "(:Person)-[:MANAGES]->(:Place)\n",
      "(:Person)-[:CHILD_OF]->(:Person)\n",
      "(:Person)-[:OWNS]->(:Place)\n",
      "(:Person)-[:OWNS]->(:Restaurant)\n",
      "(:Person)-[:MARRIED_TO]->(:Person)\n",
      "(:Person)-[:SPEARHEADED]->(:Initiative)\n",
      "(:Person)-[:COLLABORATED]->(:Organization)\n",
      "(:Person)-[:OCCUPIED]->(:Place)\n",
      "(:Person)-[:SISTER]->(:Person)\n",
      "(:Person)-[:GRANDMOTHER]->(:Person)\n",
      "(:Person)-[:MATRIARCH]->(:Family)\n",
      "(:Person)-[:MENTOR]->(:Food)\n",
      "(:Person)-[:MENTOR]->(:Person)\n",
      "(:Person)-[:OWNER]->(:Restaurant)\n",
      "(:Family)-[:EXPANSION]->(:Location)\n",
      "(:Family)-[:ORIGIN]->(:Location)\n",
      "(:Family)-[:FAMILY_MEMBER]->(:Person)\n",
      "(:Place)-[:SERVED]->(:Persongroup)\n",
      "(:Restaurant)-[:LOCATED_IN]->(:Location)\n",
      "(:Organization)-[:EXPANSION]->(:Location)\n",
      "(:Organization)-[:ORIGIN]->(:Location)\n",
      "(:Organization)-[:FAMILY_MEMBER]->(:Person)\n",
      "(:Organization)-[:CELEBRATE]->(:Culture)\n",
      "(:Organization)-[:SHOWCASE]->(:Cuisine)\n",
      "(:Organization)-[:ADVOCATE]->(:Person)\n",
      "(:Organization)-[:OWNED_BY]->(:Place)\n",
      "(:Organization)-[:ORGANIZED]->(:Event)\n",
      "(:Initiative)-[:PROMOTED]->(:Concept)\n",
      "(:Initiative)-[:SUPPORTED]->(:Concept)\n",
      "(:Concept)-[:RELATED]->(:Exchange)\n",
      "(:Concept)-[:CELEBRATE]->(:Organization)\n",
      "(:Event)-[:DONATED_BY]->(:Persongroup)\n",
      "(:Event)-[:DONATED_BY]->(:Organizationgroup)\n",
      "(:Event)-[:OCCUR_AT]->(:Organization)\n",
      "(:Market)-[:OCCUR_AT]->(:Organization)\n",
      "(:Exchange)-[:PROMOTE]->(:Organization)\n"
     ]
    }
   ],
   "source": [
    "#Display Neo4j Database Schema\n",
    "# https://python.langchain.com/docs/tutorials/graph/\n",
    "graph.refresh_schema()\n",
    "# print(graph.schema)\n",
    "enhanced_graph = Neo4jGraph(enhanced_schema=True)\n",
    "#more details\n",
    "print(enhanced_graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Person {id: 'Giovanni Caruso'}) RETURN p\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'p': {'id': 'Giovanni Caruso'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who is Giovanni Caruso',\n",
       " 'result': 'Giovanni Caruso is an individual whose identity was provided in the context.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/graph/\n",
    "\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True\n",
    ")\n",
    "response = chain.invoke({\"query\": \"Who is Giovanni Caruso\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# his method configures keyword and vector search indexes for a hybrid search approach, targeting nodes labeled Document. Additionally, it calculates text embedding values if they are missing.\n",
    "#  The vector index can then be called with the similarity_search method.\n",
    "#     \n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    ")\n",
    "\n",
    "# from langchain_community.llms import LlamaCppEmbeddings\n",
    "# embeddings = LlamaCppEmbeddings(model_path=\"/home/ubuntu/repos/gguf/mxbai-embed-large-v1-f16.gguf\",\n",
    "#                               n_ctx=4096,\n",
    "#                               n_gpu_layers=-1,\n",
    "#                             #   n_threads=8,\n",
    "#                             #   n_batch=1000\n",
    "#                               )\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph retriever starts by identifying relevant entities in the input. For simplicity, we instruct the LLM to identify people, organizations, and locations. To achieve this, we will use LCEL with the newly added with_structured_output method to achieve this.\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Great, now that we can detect entities in the question, let's use a full-text index to map them to the knowledge graph. First, we need to define a full-text index and a function that will generate full-text queries that allow a bit of misspelling, which we won't go into much detail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX `fulltext_entity_id` IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke(question)\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured_retriever function starts by detecting entities in the user question. Next, it iterates over the detected entities and uses a Cypher template to retrieve the neighborhood of relevant nodes. Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucia - OCCUPIED -> Kitchen\n",
      "Lucia - GRANDMOTHER -> Amico\n",
      "Lucia - MENTOR -> Fresh_Pasta\n",
      "Lucia - SISTER -> Antonio\n",
      "Lucia - MENTOR -> Caponata\n",
      "Lucia - MENTOR -> Amico\n",
      "Lucia - MATRIARCH -> Caruso_Family\n",
      "Pietro - RELATIVE_OF -> Nonna Lucia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\james\\AppData\\Local\\Temp\\ipykernel_4224\\133239167.py:12: LangChainDeprecationWarning: The function `remove_lucene_chars` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the function exists in the :meth:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :meth:`~langchain-neo4j` and import as `from :meth:`~langchain_neo4j.vectorstores.neo4j_vector import remove_lucene_chars``.\n",
      "  words = [el for el in remove_lucene_chars(input).split() if el]\n"
     ]
    }
   ],
   "source": [
    "print(structured_retriever(\"Who is Lucia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the unstructured and graph retriever to create the final context that will be passed to an LLM.\n",
    "\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    "    ConfigurableField\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#Follow up Question prompts\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOllama(model = llm_model, temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Who is most powerful member in the family?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nonna Lucia, as the matriarch and mentor of the Caruso family, is considered the most powerful member due to her influence on the family's culinary traditions, values, and wisdom.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " chain.invoke({\"question\": \"Who is most powerful member in the family?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

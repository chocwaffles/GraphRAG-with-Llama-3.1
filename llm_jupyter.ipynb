{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/how_to/#extraction\n",
    "# https://python.langchain.com/docs/tutorials/extraction/\n",
    "%pip install --upgrade --quiet typing langchain langchain_core pydantic langchain-community langchain_experimental langchain_ollama neo4j langchain_neo4j os yfiles_jupyter_graphs langchain_ollama llama-cpp-python json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate neo4j instance.\n",
    "\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "graph = Neo4jGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Schema\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(\n",
    "        default=None, description=\"The color of the person's hair if known\"\n",
    "    )\n",
    "    height_in_meters: Optional[str] = Field(\n",
    "        default=None, description=\"Height measured in meters\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the extractor\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        # Please see the how-to about improving performance with\n",
    "        # reference examples.\n",
    "        # MessagesPlaceholder('examples'),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load LLM\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "llm_model=\"mistral:7b-instruct-v0.3-q8_0\" #mistral:7b-instruct-v0.3-q8_0, llama3.1:8b-instruct-q8_0, llama3.2:3b-instruct-fp16\n",
    "\n",
    "llm = OllamaFunctions(model=llm_model, temperature=0, format=\"json\")\n",
    "# llm = ChatOllama(model=llm_model, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading via LlammaCPP\n",
    "# # from langchain_community.llms import LlamaCpp\n",
    "# from langchain_community.chat_models import ChatLlamaCpp\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "# # https://github.com/langchain-ai/langchain/issues/25318\n",
    "# # https://github.com/langchain-ai/langchainjs/issues/6051\n",
    "# llm = ChatLlamaCpp(\n",
    "#     model_path=\"/home/ubuntu/repos/gguf/llama3_1_8b_f16.gguf\", #\"/home/ubuntu/repos/gguf/llama3_2_3b_q8_0.gguf\", mistral_7b_f16.gguf, llama3_1_8b_f16.gguf\n",
    "#     n_gpu_layers=-1,\n",
    "#     temperature=0,\n",
    "#     echo='True',\n",
    "#     grammar='json',\n",
    "#     n_batch=1,\n",
    "#     n_ctx=2048,\n",
    "#     f16_kv=False,\n",
    "#     # callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#     # verbose=True,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Jeff', hair_color='black', height_in_meters=None), Person(name='Anna', hair_color='black', height_in_meters=None)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load text instead\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(file_path=\"dummytext.txt\")\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100) #or TokenTextSplitter\n",
    "documents = text_splitter.split_documents(documents=docs)\n",
    "\n",
    "#see https://python.langchain.com/docs/how_to/extraction_long_text/ for more efficient parallel runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert raw unstructured text into graph documents.\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "llm_transformer = LLMGraphTransformer(llm=llm) #, ignore_tool_usage=True\n",
    "\n",
    "#if filtering is required.\n",
    "# llm_transformer_filtered = LLMGraphTransformer(\n",
    "#     llm=llm,\n",
    "#     allowed_nodes=[\"Person\", \"Country\", \"Organization\"],\n",
    "#     allowed_relationships=[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n",
    "# )\n",
    "# graph_documents_filtered = llm_transformer_filtered.convert_to_graph_documents(\n",
    "#     documents\n",
    "# )\n",
    "# print(f\"Nodes:{graph_documents_filtered[0].nodes}\")\n",
    "# print(f\"Relationships:{graph_documents_filtered[0].relationships}\")\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_documents:nodes=[Node(id='Maria', type='Person', properties={}), Node(id='Caruso family', type='Family', properties={}), Node(id='Giovanni Caruso', type='Person', properties={}), Node(id='culinary heritage', type='Culinary Tradition', properties={}), Node(id='Sicily', type='Location', properties={}), Node(id='Santa Caterina', type='Location', properties={})] relationships=[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Santa Caterina', type='Location', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='LOCATED_IN', properties={}), Relationship(source=Node(id='Caruso family', type='Family', properties={}), target=Node(id='culinary heritage', type='Culinary Tradition', properties={}), type='KEEPS', properties={})] source=Document(metadata={'source': 'dummytext.txt', 'id': '3d712ead111d00139576a93246cf5f25'}, page_content=\"1. The Story of Amicoâ€™s Family: A Legacy of Love and Tradition\\nIn the idyllic village of Santa Caterina, amidst the rolling hills and sun-kissed landscapes of Sicily, lies the genesis of the Caruso family, a lineage intertwined with the island's rich culinary tapestry. The Carusos were not mere inhabitants of the land; they were the keepers of a culinary heritage that spanned generations. Each family member contributed their unique flair, crafting a narrative of flavors that reflected their diverse experiences and deep-seated love for food.\\n\\nGiovanni Caruso and Maria: The Founding Generation\")\n",
      "Nodes:[Node(id='Maria', type='Person', properties={}), Node(id='Caruso family', type='Family', properties={}), Node(id='Giovanni Caruso', type='Person', properties={}), Node(id='culinary heritage', type='Culinary Tradition', properties={}), Node(id='Sicily', type='Location', properties={}), Node(id='Santa Caterina', type='Location', properties={})]\n",
      "Relationships:[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Santa Caterina', type='Location', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='LOCATED_IN', properties={}), Relationship(source=Node(id='Caruso family', type='Family', properties={}), target=Node(id='culinary heritage', type='Culinary Tradition', properties={}), type='KEEPS', properties={})]\n",
      "[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Caruso family', type='Family', properties={}), type='BELONGS_TO', properties={}), Relationship(source=Node(id='Santa Caterina', type='Location', properties={}), target=Node(id='Sicily', type='Location', properties={}), type='LOCATED_IN', properties={}), Relationship(source=Node(id='Caruso family', type='Family', properties={}), target=Node(id='culinary heritage', type='Culinary Tradition', properties={}), type='KEEPS', properties={})]\n",
      "[Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Maria', type='Person', properties={}), type='IS_MARRIED_TO', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='LIVES_IN', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Farmer', type='Occupation', properties={}), type='OCCUPATION', properties={}), Relationship(source=Node(id='Giovanni Caruso', type='Person', properties={}), target=Node(id='Alchemist of flavors', type='Role', properties={}), type='HAS_ROLE', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Santa Caterina', type='Location', properties={}), type='LIVES_IN', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Cook', type='Occupation', properties={}), type='OCCUPATION', properties={}), Relationship(source=Node(id='Maria', type='Person', properties={}), target=Node(id='Soul of the kitchen', type='Role', properties={}), type='HAS_ROLE', properties={})]\n"
     ]
    }
   ],
   "source": [
    "#view the contents\n",
    "print(f\"graph_documents:{graph_documents[0]}\")\n",
    "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships:{graph_documents[0].relationships}\")\n",
    "\n",
    "doc = graph_documents[0:2]\n",
    "non_empty_nodes = [doc for doc in doc if doc.nodes]\n",
    "non_empty_rel = [doc for doc in doc if doc.relationships]\n",
    "\n",
    "# for graph_document in non_empty_nodes:\n",
    "#     print(graph_document.nodes)\n",
    "\n",
    "for doc in non_empty_rel:\n",
    "    print(doc.relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "# Add contents into neo4j database\n",
    "driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"], os.environ[\"NEO4J_PASSWORD\"])\n",
    "    )\n",
    "\n",
    "def clear_database(tx):\n",
    "    tx.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n\n",
    "    \"\"\")    \n",
    "\n",
    "with driver.session() as session:\n",
    "    session.execute_write(clear_database)\n",
    "\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2618910b034d96add4699ba30e42a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase, Driver\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r]->(t) WHERE type(r) <> 'MENTIONS'RETURN s,r,t\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    #display(widget)\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "- **Document**\n",
      "  - `id`: STRING Example: \"3d712ead111d00139576a93246cf5f25\"\n",
      "  - `source`: STRING Available options: ['dummytext.txt']\n",
      "  - `text`: STRING Example: \"1. The Story of Amicoâ€™s Family: A Legacy of Love a\"\n",
      "- **Person**\n",
      "  - `id`: STRING Example: \"Maria\"\n",
      "- **Location**\n",
      "  - `id`: STRING Available options: ['Sicily', 'Santa Caterina', 'Amalfi Coast', 'New York City', 'picturesque village of Santa Caterina', 'bustling streets of New York City']\n",
      "- **Family**\n",
      "  - `id`: STRING Available options: ['Caruso family', 'Antonio, Giovanni and Maria', 'their children', 'family', 'Caruso Family', 'The Carusos']\n",
      "- **Concept**\n",
      "  - `id`: STRING Available options: ['Tradition', 'environmental sustainability', 'benefits of consuming seasonal and organic produce']\n",
      "- **Food**\n",
      "  - `id`: STRING Available options: ['organic salads', 'Cannoli', 'gluten-free pasta', 'Lasagna', 'small plates', 'nutritious food', 'traditional Sicilian dishes', 'other international cuisines']\n",
      "- **Restaurant**\n",
      "  - `id`: STRING Available options: ['trattoria', 'Amico', 'La Dolce Vita', 'Il Mare Nostrum', 'La Terra di Siena', \"Amico's\", 'Bella Vita', \"The Carusos' restaurants\"]\n",
      "- **City**\n",
      "  - `id`: STRING Available options: ['Rome']\n",
      "- **Landmark**\n",
      "  - `id`: STRING Available options: ['Trevi Fountain']\n",
      "- **Dish**\n",
      "  - `id`: STRING Available options: ['Cacio e Pepe', 'Carbonara', 'Spaghetti alle Vongole', 'Grilled Octopus with Lemon and Herbs', 'Pappardelle al Cinghiale', 'Bistecca alla Fiorentina']\n",
      "- **Region**\n",
      "  - `id`: STRING Available options: ['Tuscany']\n",
      "- **Organization**\n",
      "  - `id`: STRING Available options: ['Caruso family']\n",
      "- **Initiative**\n",
      "  - `id`: STRING Available options: [\"The Carusos' initiatives\", 'Farm-to-Table initiative', 'Food for All initiative']\n",
      "- **Event**\n",
      "  - `id`: STRING Available options: ['workshops in Rome', \"Caruso's food drives\", 'food drives', 'The annual cultural and culinary festivals', 'annual cultural and culinary festivals', 'Events that promote local talent', 'Exhibitions']\n",
      "- **Culture**\n",
      "  - `id`: STRING Available options: ['Sicilian culture']\n",
      "- **Cuisine**\n",
      "  - `id`: STRING Available options: [\"Italy's coastal cuisine\", 'Tuscan cuisine']\n",
      "- **Market**\n",
      "  - `id`: STRING Available options: ['artisan markets']\n",
      "- **Group**\n",
      "  - `id`: STRING Available options: ['his family and the village', 'local farmers and producers', 'Local performers', 'Local musicians', 'Local artisans', 'Local artists']\n",
      "- **Culinary Tradition**\n",
      "  - `id`: STRING Available options: ['culinary heritage']\n",
      "- **Role**\n",
      "  - `id`: STRING Available options: ['Alchemist of flavors', 'Soul of the kitchen', \"the village's go-to chef\"]\n",
      "- **Occupation**\n",
      "  - `id`: STRING Available options: ['Cook', 'Farmer']\n",
      "- **Story**\n",
      "  - `id`: STRING Available options: [\"tales of their ancestry and the island's history\"]\n",
      "- **Action**\n",
      "  - `id`: STRING Available options: ['dishes that were both nostalgic and avant-garde', 'traditional Sicilian flavors with inventive techni']\n",
      "- **Creation**\n",
      "  - `id`: STRING Available options: [\"Antonio Caruso's most famous creation\", 'his most famous creation']\n",
      "- **Influence**\n",
      "  - `id`: STRING Available options: [\"what would later become Amico's signature style\"]\n",
      "- **Characteristic**\n",
      "  - `id`: STRING Available options: ['his flair for innovation', 'a charismatic storyteller', 'his culinary prowess', 'warm hospitality and authentic flavors', 'live classical music', 'supports local artists', 'a hub for food, art, and culture', 'committed to sustainability', 'a creative force', 'unique background']\n",
      "- **Profession**\n",
      "  - `id`: STRING Available options: ['baker par excellence', 'skilled fisherman']\n",
      "- **Recipe Collection**\n",
      "  - `id`: STRING Available options: [\"family's recipes\"]\n",
      "- **Empire**\n",
      "  - `id`: STRING Available options: ['A Culinary Empire']\n",
      "- **Skill**\n",
      "  - `id`: STRING Available options: ['baking', 'fishing', 'their unique talents']\n",
      "- **Focus**\n",
      "  - `id`: STRING Available options: ['freshest seafood']\n",
      "- **Dessert**\n",
      "  - `id`: STRING Available options: ['Tiramisu', 'Panna Cotta']\n",
      "- **Drink**\n",
      "  - `id`: STRING Available options: ['Italian-inspired cocktails']\n",
      "- **Bar**\n",
      "  - `id`: STRING Available options: [\"Bella Vita's rooftop bar\"]\n",
      "- **Experience**\n",
      "  - `id`: STRING Available options: ['invaluable hands-on experience']\n",
      "- **Culinary Class**\n",
      "  - `id`: STRING Available options: ['classes in baking', 'classes in seafood preparation']\n",
      "- **Lesson**\n",
      "  - `id`: STRING Available options: ['aspiring chefs the importance of using organic and']\n",
      "- **Social Cause**\n",
      "  - `id`: STRING Available options: ['social causes']\n",
      "- **Cooking Practice**\n",
      "  - `id`: STRING Available options: ['sustainable cooking practices']\n",
      "- **Support**\n",
      "  - `id`: STRING Available options: ['guidance and support to young chefs from diverse b']\n",
      "- **Practice**\n",
      "  - `id`: STRING Available options: ['sustainable and ethical food practices']\n",
      "- **Economy**\n",
      "  - `id`: STRING Available options: ['local economy']\n",
      "- **Ingredient**\n",
      "  - `id`: STRING Available options: ['locally sourced ingredients']\n",
      "- **Geographical Location**\n",
      "  - `id`: STRING Available options: ['cities']\n",
      "- **Business**\n",
      "  - `id`: STRING Available options: ['restaurants', \"Caruso's restaurants\", 'local businesses', 'culinary empire']\n",
      "- **Resource**\n",
      "  - `id`: STRING Available options: ['skills and resources']\n",
      "- **Kitchen**\n",
      "  - `id`: STRING Available options: [\"Caruso's community kitchens\"]\n",
      "- **Donation**\n",
      "  - `id`: STRING Available options: ['donations']\n",
      "- **Service**\n",
      "  - `id`: STRING Available options: ['free meals']\n",
      "- **Platform**\n",
      "  - `id`: STRING Available options: ['a platform for cultural exchange']\n",
      "- **Demonstration**\n",
      "  - `id`: STRING Available options: ['cooking demonstrations']\n",
      "- **Performance**\n",
      "  - `id`: STRING Available options: ['live performances']\n",
      "- **Atmosphere**\n",
      "  - `id`: STRING Available options: ['a vibrant and inclusive atmosphere']\n",
      "- **Activity**\n",
      "  - `id`: STRING Available options: ['community involvement']\n",
      "- **Belief**\n",
      "  - `id`: STRING Available options: ['the power of food to connect people']\n",
      "- **Goal**\n",
      "  - `id`: STRING Available options: ['make a positive impact']\n",
      "- **Cause**\n",
      "  - `id`: STRING Available options: ['social causes']\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Document)-[:MENTIONS]->(:Location)\n",
      "(:Document)-[:MENTIONS]->(:Culinary Tradition)\n",
      "(:Document)-[:MENTIONS]->(:Person)\n",
      "(:Document)-[:MENTIONS]->(:Family)\n",
      "(:Document)-[:MENTIONS]->(:Organization)\n",
      "(:Document)-[:MENTIONS]->(:Role)\n",
      "(:Document)-[:MENTIONS]->(:Occupation)\n",
      "(:Document)-[:MENTIONS]->(:Characteristic)\n",
      "(:Document)-[:MENTIONS]->(:Group)\n",
      "(:Document)-[:MENTIONS]->(:Action)\n",
      "(:Document)-[:MENTIONS]->(:Story)\n",
      "(:Document)-[:MENTIONS]->(:Creation)\n",
      "(:Document)-[:MENTIONS]->(:Influence)\n",
      "(:Document)-[:MENTIONS]->(:Concept)\n",
      "(:Document)-[:MENTIONS]->(:Profession)\n",
      "(:Document)-[:MENTIONS]->(:Restaurant)\n",
      "(:Document)-[:MENTIONS]->(:Culture)\n",
      "(:Document)-[:MENTIONS]->(:Recipe Collection)\n",
      "(:Document)-[:MENTIONS]->(:Empire)\n",
      "(:Document)-[:MENTIONS]->(:City)\n",
      "(:Document)-[:MENTIONS]->(:Dish)\n",
      "(:Document)-[:MENTIONS]->(:Landmark)\n",
      "(:Document)-[:MENTIONS]->(:Skill)\n",
      "(:Document)-[:MENTIONS]->(:Cuisine)\n",
      "(:Document)-[:MENTIONS]->(:Focus)\n",
      "(:Document)-[:MENTIONS]->(:Region)\n",
      "(:Document)-[:MENTIONS]->(:Dessert)\n",
      "(:Document)-[:MENTIONS]->(:Food)\n",
      "(:Document)-[:MENTIONS]->(:Bar)\n",
      "(:Document)-[:MENTIONS]->(:Drink)\n",
      "(:Document)-[:MENTIONS]->(:Experience)\n",
      "(:Document)-[:MENTIONS]->(:Cooking Practice)\n",
      "(:Document)-[:MENTIONS]->(:Culinary Class)\n",
      "(:Document)-[:MENTIONS]->(:Social Cause)\n",
      "(:Document)-[:MENTIONS]->(:Cause)\n",
      "(:Document)-[:MENTIONS]->(:Lesson)\n",
      "(:Document)-[:MENTIONS]->(:Event)\n",
      "(:Document)-[:MENTIONS]->(:Support)\n",
      "(:Document)-[:MENTIONS]->(:Ingredient)\n",
      "(:Document)-[:MENTIONS]->(:Initiative)\n",
      "(:Document)-[:MENTIONS]->(:Economy)\n",
      "(:Document)-[:MENTIONS]->(:Practice)\n",
      "(:Document)-[:MENTIONS]->(:Business)\n",
      "(:Document)-[:MENTIONS]->(:Resource)\n",
      "(:Document)-[:MENTIONS]->(:Geographical Location)\n",
      "(:Document)-[:MENTIONS]->(:Service)\n",
      "(:Document)-[:MENTIONS]->(:Donation)\n",
      "(:Document)-[:MENTIONS]->(:Kitchen)\n",
      "(:Document)-[:MENTIONS]->(:Platform)\n",
      "(:Document)-[:MENTIONS]->(:Atmosphere)\n",
      "(:Document)-[:MENTIONS]->(:Market)\n",
      "(:Document)-[:MENTIONS]->(:Performance)\n",
      "(:Document)-[:MENTIONS]->(:Demonstration)\n",
      "(:Document)-[:MENTIONS]->(:Belief)\n",
      "(:Document)-[:MENTIONS]->(:Goal)\n",
      "(:Document)-[:MENTIONS]->(:Activity)\n",
      "(:Person)-[:HAS_ROLE]->(:Role)\n",
      "(:Person)-[:HAS_ROLE]->(:Profession)\n",
      "(:Person)-[:BELONGS_TO]->(:Family)\n",
      "(:Person)-[:BELONGS_TO]->(:Organization)\n",
      "(:Person)-[:LIVES_IN]->(:Location)\n",
      "(:Person)-[:OCCUPATION]->(:Occupation)\n",
      "(:Person)-[:IS_MARRIED_TO]->(:Person)\n",
      "(:Person)-[:INSTILLED_VALUES_IN]->(:Family)\n",
      "(:Person)-[:ARE_GUARDIANS_OF]->(:Concept)\n",
      "(:Person)-[:RUN]->(:Restaurant)\n",
      "(:Person)-[:OWNS]->(:Restaurant)\n",
      "(:Person)-[:CAPTIVATED]->(:Group)\n",
      "(:Person)-[:CREATED]->(:Action)\n",
      "(:Person)-[:IS_CHILD_OF]->(:Family)\n",
      "(:Person)-[:IS_CHILD_OF]->(:Person)\n",
      "(:Person)-[:INHERITED_PASSION_FROM]->(:Person)\n",
      "(:Person)-[:ADDED_FLAIR_FOR_INNOVATION]->(:Characteristic)\n",
      "(:Person)-[:KNOWN_AS]->(:Characteristic)\n",
      "(:Person)-[:WAS_KNOWN_FOR]->(:Story)\n",
      "(:Person)-[:HAD_UNMATCHED_COOKING_PROWESS]->(:Characteristic)\n",
      "(:Person)-[:BLENDED]->(:Action)\n",
      "(:Person)-[:BECAME_GO_TO_CHEF]->(:Role)\n",
      "(:Person)-[:CREATED_FAMOUSLY]->(:Creation)\n",
      "(:Person)-[:IS_RELATED_TO]->(:Person)\n",
      "(:Person)-[:OFFERED]->(:Culinary Class)\n",
      "(:Person)-[:MARRIED_TO]->(:Person)\n",
      "(:Person)-[:HOSTED]->(:Event)\n",
      "(:Person)-[:OFFERS_ON_MENU]->(:Dish)\n",
      "(:Person)-[:IS_A]->(:Profession)\n",
      "(:Person)-[:HOSTS]->(:Event)\n",
      "(:Person)-[:SPEARHEADED]->(:Initiative)\n",
      "(:Person)-[:COLLABORATED_WITH]->(:Group)\n",
      "(:Person)-[:TAUGHT]->(:Lesson)\n",
      "(:Person)-[:SISTER_OF]->(:Person)\n",
      "(:Person)-[:GRANDMOTHER_OF]->(:Person)\n",
      "(:Person)-[:GRANDMOTHER_OF]->(:Restaurant)\n",
      "(:Person)-[:MATRIARCH_OF]->(:Family)\n",
      "(:Person)-[:MATRIARCH_OF]->(:Organization)\n",
      "(:Person)-[:CUSTODIAN_OF]->(:Recipe Collection)\n",
      "(:Person)-[:TEACHER_OF]->(:Person)\n",
      "(:Person)-[:TEACHER_OF]->(:Restaurant)\n",
      "(:Person)-[:FOCUSED_ON]->(:Cooking Practice)\n",
      "(:Person)-[:GUIDE_OF]->(:Family)\n",
      "(:Person)-[:CREATES]->(:Dessert)\n",
      "(:Person)-[:HAS_SKILL]->(:Skill)\n",
      "(:Person)-[:HAS_CHARACTERISTIC]->(:Characteristic)\n",
      "(:Person)-[:ORGANIZED]->(:Event)\n",
      "(:Person)-[:ADVOCATED_FOR]->(:Practice)\n",
      "(:Person)-[:RAISED_AWARENESS]->(:Concept)\n",
      "(:Person)-[:FEATURING]->(:Market)\n",
      "(:Person)-[:FEATURING]->(:Performance)\n",
      "(:Person)-[:FEATURING]->(:Demonstration)\n",
      "(:Person)-[:OWNED]->(:Business)\n",
      "(:Person)-[:LAUNCHED]->(:Initiative)\n",
      "(:Person)-[:HAS_UNIQUE_BACKGROUND]->(:Characteristic)\n",
      "(:Person)-[:HAS_TALENTS]->(:Skill)\n",
      "(:Location)-[:LOCATED_IN]->(:Location)\n",
      "(:Location)-[:LOCATED_IN]->(:Restaurant)\n",
      "(:Family)-[:BELIEVES_IN]->(:Goal)\n",
      "(:Family)-[:BELIEVES_IN]->(:Social Cause)\n",
      "(:Family)-[:BELIEVES_IN]->(:Cause)\n",
      "(:Family)-[:BELIEVES_IN]->(:Activity)\n",
      "(:Family)-[:BELIEVES_IN]->(:Belief)\n",
      "(:Family)-[:CONTRIBUTE]->(:Resource)\n",
      "(:Family)-[:HELP]->(:Person)\n",
      "(:Family)-[:SUPPORTS]->(:Group)\n",
      "(:Family)-[:COLLABORATES_WITH]->(:Group)\n",
      "(:Family)-[:HAS_BACKGROUND]->(:Location)\n",
      "(:Family)-[:HAS_LEGACY]->(:Business)\n",
      "(:Family)-[:KEEPS]->(:Culinary Tradition)\n",
      "(:Family)-[:COMMITTED_TO]->(:Social Cause)\n",
      "(:Family)-[:COMMITTED_TO]->(:Cause)\n",
      "(:Family)-[:OFFERED]->(:Support)\n",
      "(:Family)-[:PROVIDED]->(:Experience)\n",
      "(:Family)-[:OWNED]->(:Restaurant)\n",
      "(:Family)-[:OWNS]->(:Empire)\n",
      "(:Family)-[:ORGANIZED]->(:Event)\n",
      "(:Family)-[:ADVOCATED_FOR]->(:Practice)\n",
      "(:Family)-[:RAISED_AWARENESS]->(:Concept)\n",
      "(:Family)-[:FEATURING]->(:Market)\n",
      "(:Family)-[:FEATURING]->(:Performance)\n",
      "(:Family)-[:FEATURING]->(:Demonstration)\n",
      "(:Restaurant)-[:KNOWN_FOR]->(:Characteristic)\n",
      "(:Restaurant)-[:IS_A_MICROCOSM]->(:Culture)\n",
      "(:Restaurant)-[:HOSTS]->(:Event)\n",
      "(:Restaurant)-[:SPEARHEADED]->(:Initiative)\n",
      "(:Restaurant)-[:COLLABORATED_WITH]->(:Group)\n",
      "(:Restaurant)-[:LOCATED_NEAR]->(:Landmark)\n",
      "(:Restaurant)-[:OWNED_BY]->(:Person)\n",
      "(:Restaurant)-[:LOCATED_IN]->(:City)\n",
      "(:Restaurant)-[:LOCATED_IN]->(:Region)\n",
      "(:Restaurant)-[:HAS_AMBIANCE]->(:Characteristic)\n",
      "(:Restaurant)-[:HAS_EXPERTISE]->(:Skill)\n",
      "(:Restaurant)-[:HAS_EXPERTISE]->(:Person)\n",
      "(:Restaurant)-[:LOCATED_AT]->(:Location)\n",
      "(:Restaurant)-[:SERVES]->(:Dish)\n",
      "(:Restaurant)-[:SERVES]->(:Cuisine)\n",
      "(:Restaurant)-[:HAS_FOCUS]->(:Focus)\n",
      "(:Restaurant)-[:BELONGS_TO]->(:Family)\n",
      "(:Restaurant)-[:BELONGS_TO]->(:Organization)\n",
      "(:Restaurant)-[:OFFERS]->(:Food)\n",
      "(:Restaurant)-[:HAS_CHARACTERISTIC]->(:Characteristic)\n",
      "(:Restaurant)-[:USED]->(:Ingredient)\n",
      "(:Organization)-[:BELIEVES_IN]->(:Goal)\n",
      "(:Organization)-[:BELIEVES_IN]->(:Social Cause)\n",
      "(:Organization)-[:BELIEVES_IN]->(:Cause)\n",
      "(:Organization)-[:BELIEVES_IN]->(:Activity)\n",
      "(:Organization)-[:BELIEVES_IN]->(:Belief)\n",
      "(:Organization)-[:CONTRIBUTE]->(:Resource)\n",
      "(:Organization)-[:HELP]->(:Person)\n",
      "(:Organization)-[:SUPPORTS]->(:Group)\n",
      "(:Organization)-[:COLLABORATES_WITH]->(:Group)\n",
      "(:Organization)-[:HAS_BACKGROUND]->(:Location)\n",
      "(:Organization)-[:HAS_LEGACY]->(:Business)\n",
      "(:Organization)-[:KEEPS]->(:Culinary Tradition)\n",
      "(:Organization)-[:COMMITTED_TO]->(:Social Cause)\n",
      "(:Organization)-[:COMMITTED_TO]->(:Cause)\n",
      "(:Organization)-[:OFFERED]->(:Support)\n",
      "(:Organization)-[:PROVIDED]->(:Experience)\n",
      "(:Initiative)-[:SUPPORTED]->(:Economy)\n",
      "(:Initiative)-[:PROMOTED]->(:Concept)\n",
      "(:Event)-[:COLLECT]->(:Donation)\n",
      "(:Event)-[:RECEIVE]->(:Business)\n",
      "(:Event)-[:RECEIVE]->(:Person)\n",
      "(:Event)-[:SHOWCASED]->(:Food)\n",
      "(:Event)-[:CREATED]->(:Atmosphere)\n",
      "(:Event)-[:SERVED_AS]->(:Platform)\n",
      "(:Creation)-[:LAID_GROUNDWORK_FOR]->(:Influence)\n",
      "(:Bar)-[:OFFERS]->(:Food)\n",
      "(:Bar)-[:OFFERS]->(:Drink)\n",
      "(:Business)-[:HAVE_LOCATIONS]->(:Geographical Location)\n",
      "(:Kitchen)-[:PROVIDE]->(:Service)\n",
      "(:Kitchen)-[:PROVIDE]->(:Food)\n",
      "(:Kitchen)-[:SERVE]->(:Person)\n"
     ]
    }
   ],
   "source": [
    "#Display Neo4j Database Schema\n",
    "# https://python.langchain.com/docs/tutorials/graph/\n",
    "graph.refresh_schema()\n",
    "# print(graph.schema)\n",
    "enhanced_graph = Neo4jGraph(enhanced_schema=True)\n",
    "#more details\n",
    "print(enhanced_graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to parse a response from mistral:7b-instruct-v0.3-q8_0 output: {\n    \"Lucia\": {\n      \"type\": \"Family\",\n      \"name\": \"Lucia\"\n    }\n  }",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_neo4j\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphCypherQAChain\n\u001b[1;32m      5\u001b[0m chain \u001b[38;5;241m=\u001b[39m GraphCypherQAChain\u001b[38;5;241m.\u001b[39mfrom_llm(\n\u001b[1;32m      6\u001b[0m     graph\u001b[38;5;241m=\u001b[39menhanced_graph, llm\u001b[38;5;241m=\u001b[39mllm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dangerous_requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is Lucia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m response\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_neo4j/chains/graph_qa/cypher.py:352\u001b[0m, in \u001b[0;36mGraphCypherQAChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    348\u001b[0m args\u001b[38;5;241m.\u001b[39mupdate(inputs)\n\u001b[1;32m    350\u001b[0m intermediate_steps: List \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 352\u001b[0m generated_cypher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcypher_generation_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Extract Cypher code if it is wrapped in backticks\u001b[39;00m\n\u001b[1;32m    355\u001b[0m generated_cypher \u001b[38;5;241m=\u001b[39m extract_cypher(generated_cypher)\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    607\u001b[0m         _output_key\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    612\u001b[0m         _output_key\n\u001b[1;32m    613\u001b[0m     ]\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/repos/langchain/.venv/lib/python3.10/site-packages/langchain_experimental/llms/ollama_functions.py:345\u001b[0m, in \u001b[0;36mOllamaFunctions._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m         response \u001b[38;5;241m=\u001b[39m parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    346\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse a response from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m output: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_generation_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m         )\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatResult(\n\u001b[1;32m    350\u001b[0m         generations\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    351\u001b[0m             ChatGeneration(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m         ]\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    359\u001b[0m called_tool_arguments \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    360\u001b[0m     parsed_chat_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_input\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parsed_chat_result\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    363\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse a response from mistral:7b-instruct-v0.3-q8_0 output: {\n    \"Lucia\": {\n      \"type\": \"Family\",\n      \"name\": \"Lucia\"\n    }\n  }"
     ]
    }
   ],
   "source": [
    "# https://python.langchain.com/docs/tutorials/graph/\n",
    "\n",
    "from langchain_neo4j import GraphCypherQAChain\n",
    "\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    graph=enhanced_graph, llm=llm, verbose=True, allow_dangerous_requests=True\n",
    ")\n",
    "response = chain.invoke({\"query\": \"Who is Lucia\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# his method configures keyword and vector search indexes for a hybrid search approach, targeting nodes labeled Document. Additionally, it calculates text embedding values if they are missing.\n",
    "#  The vector index can then be called with the similarity_search method.\n",
    "#     \n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"mxbai-embed-large\",\n",
    ")\n",
    "\n",
    "# from langchain_community.llms import LlamaCppEmbeddings\n",
    "# embeddings = LlamaCppEmbeddings(model_path=\"/home/ubuntu/repos/gguf/mxbai-embed-large-v1-f16.gguf\",\n",
    "#                               n_ctx=4096,\n",
    "#                               n_gpu_layers=-1,\n",
    "#                             #   n_threads=8,\n",
    "#                             #   n_batch=1000\n",
    "#                               )\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    embeddings,\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")\n",
    "\n",
    "vector_retriever = vector_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph retriever starts by identifying relevant entities in the input. For simplicity, we instruct the LLM to identify people, organizations, and locations. To achieve this, we will use LCEL with the newly added with_structured_output method to achieve this.\n",
    "\n",
    "# Extract entities from text\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are extracting organization and person entities from the text.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Great, now that we can detect entities in the question, let's use a full-text index to map them to the knowledge graph. First, we need to define a full-text index and a function that will generate full-text queries that allow a bit of misspelling, which we won't go into much detail here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX `fulltext_entity_id` IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")\n",
    "\n",
    "# Fulltext index query\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke(question)\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('fulltext_entity_id', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "              WITH node\n",
    "              MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "              RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "              UNION ALL\n",
    "              WITH node\n",
    "              MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "              RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured_retriever function starts by detecting entities in the user question. Next, it iterates over the detected entities and uses a Cypher template to retrieve the neighborhood of relevant nodes. Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_retriever(\"Who is Giuseppe Genco Russo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the unstructured and graph retriever to create the final context that will be passed to an LLM.\n",
    "\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Follow up Question prompts\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(_template)\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOllama(model = llm_model, temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " chain.invoke({\"question\": \"Who is most powerful mafia family?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
